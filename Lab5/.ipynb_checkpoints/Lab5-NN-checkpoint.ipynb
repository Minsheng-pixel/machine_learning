{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('intro-ml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a6f2612307e1adba91018c22c4080b9427b99100a33da5d874a73d3fb277e0ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "source": [
    "# Neural Networks with PyTorch\n",
    "\n",
    "[PyTorch](https://pytorch.org) is a machine learning library, most often used for training neural networks via backpropagation. It is, however, a general purpose computing library with support for hardware accelerators like the GPUs. Installing should be as simple running the following command in your virtual environment.\n",
    "\n",
    "```bash\n",
    "> pip install torch\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PyTorch Tensors\n",
    "\n",
    "The key unit of computation in PyTorch is the `Tensor` object. In many ways, it will feel similar to `NumPy` arrays. Below are a few examples of simple operations, including converting an `np.array` into `torch.Tensor`. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.random.randn(5,3)).float()  ## or could use .double() for 64-bit precision\n",
    "print(x.shape)\n",
    "print(x.dtype)\n",
    "print(x.device)\n",
    "x"
   ]
  },
  {
   "source": [
    "As a first difference, we now have an additional `.device` attribute which defines where the tensor is stored and operated on. `cpu` tells us that the tensor will be stored on the usual on-device memory (RAM).\n",
    "\n",
    "All the usual indexing rules seen with `numpy` generally apply."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usual indexing applies.\n",
    "print(x[0])\n",
    "print(x[:2, :2])"
   ]
  },
  {
   "source": [
    "PyTorch also has all the expected utility functions to create tensors. All the typical unary/binary operations, and familiar broadcasting rules from `numpy` apply here directly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn_like(x)  ## Equivalent to torch.randn(*x.shape).float()\n",
    "x * y"
   ]
  },
  {
   "source": [
    "`torch.Tensor` API allows chaining of operations (which makes the code more readable). For instance consider here the matrix multiplication between the two tensors $X \\in \\mathbb{R}^{5\\times 3}$ and $Y \\in \\mathbb{R}^{5\\times 3}$, as $Z = X^TY \\in \\mathbb{R}^{3\\times 3}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.transpose(1, 0).matmul(y)  ## Equivalent to torch.matmul(x.transpose(1, 0), y)\n",
    "print(z.shape)\n",
    "\n",
    "## We can also go back to NumPy, if needed. Need to make sure the tensor is on CPU before the conversion.\n",
    "print(z.cpu().numpy())\n",
    "\n",
    "z"
   ]
  },
  {
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "Automatic Differentiation (AD) is the key workhorse of training modern neural networks. In essense, it is simply an efficient implementation of the chain rule from calculus on an abstraction called the _computational graph_. PyTorch does automatic differentiation through its `autograd` module on top of computational graphs that are built dynamically (i.e. at runtime).\n",
    "\n",
    "For instance, consider the function\n",
    "\n",
    "$$\n",
    "f_\\mathbf{w}(\\mathbf{x}) = \\sigma(\\mathbf{w}^T\\mathbf{x})\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function. We will compute its derivative with respect to $\\mathbf{w}$ using automatic differentiation in PyTorch.\n",
    "\n",
    "$$\n",
    "\\frac{d}{d \\mathbf{w}}f_\\mathbf{w}(\\mathbf{x}) = \\sigma(\\mathbf{w}^T\\mathbf{x})\\cdot(1-\\sigma(\\mathbf{w}^T\\mathbf{x}))\\cdot\\mathbf{x}\n",
    "$$\n",
    "\n",
    "The key method of note will be `.requires_grad_()`. This sets the tensor up to store gradients during \"backpropagation\". In practice, we often don't need gradients for all tensors in the computational graph and \n",
    "flagging some of those tensors to not store gradients can save a lot of memory consumption."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Assign random 2-D values to both tensors, and make w differentiable\n",
    "\n",
    "w = (2. * torch.randn(2)).float().requires_grad_(True)\n",
    "x = (2. * torch.randn(2)).float().requires_grad_(False)\n",
    "f = w.dot(x).sigmoid()\n",
    "f"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Computing backpropagation for most use-cases amounts to calling `.backward()` on the resulting tensor.\n",
    "\n",
    "**Warning**: Running the next cell twice will throw an error because the computational graph is destroyed once the gradients have been computed (more precisely vector-Jacobian products). We can keep the graph around, if we do need it (e.g. for higher-order gradients). For now, to re-run the backward call, first re-run the previous cell that creates `f`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_grad_w = w.grad\n",
    "with torch.no_grad():\n",
    "    analytic_grad_w = f * (1 - f) * x\n",
    "\n",
    "torch.allclose(auto_grad_w, analytic_grad_w)"
   ]
  },
  {
   "source": [
    "We see that the analytic and the automatically computed gradient are the same. In fact, automatic differentiation computes _exact_ gradients for complex functions, by simply building on top of the derivatives for primitive functions like addition, multiplication, logarithms etc.\n",
    "\n",
    "This abstraction is powerful because now we don't need to worry about computing derivatives by hand for large and complex functions like those induced by neural networks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Linear Regression\n",
    "\n",
    "Neural networks are general parametric function approximators. Any parametric function can therefore be described as a neural network. Therefore, unsurprisingly, we have been using a neural network all along when we were working with linear regression. This is a neural network with no hidden layers (the input directly connects to the output). As a reminder, linear regression constructs the function\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{w}}(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}\n",
    "$$\n",
    "\n",
    "where we assume $\\mathbf{w}$ also contains the bias term (with appropriate augmentation for the input $\\mathbf{x}$).\n",
    "\n",
    "In PyTorch, we build neural networks using the `nn.Module` abstraction. The `nn` module has plenty of objects that are equivalent to layers in neural networks. A first linear regression module is below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super().__init__()\n",
    "\n",
    "        ## We assume scalar-valued regression, i.e. output size is always 1\n",
    "        self.fc = nn.Linear(in_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "By default parameters are randomly initialized in these modules and can be accessed via the `.parameters()` generator. By default, all `nn.Parameter` containers have `requires_grad` set as `True`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LinearRegression(in_size=2)\n",
    "for p in net.parameters():\n",
    "    print(p.shape, p, p.requires_grad)"
   ]
  },
  {
   "source": [
    "### Batching\n",
    "\n",
    "A key feature of tensor libraries like PyTorch is that we can leverage parallelized computations. This means that all neural network modules can take multiple inputs, instead of just a single one. We specify multiple inputs across the \"batch dimensions\". This will be most clear with the following examples."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(100, 2)\n",
    "net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(20, 30, 100, 2)\n",
    "net(x).shape"
   ]
  },
  {
   "source": [
    "Notably, all dimensions are consumed starting from the right to the left. In this case, since `in_size=2`, and the module contains a `nn.Linear` layer, the last dimension is consumed for the matrix multiplication of batched inputs with the parameter vector. All the remaining dimensions of the input are considered as _batch dimensions_. Of course, while this behavior is true for all code PyTorch layers, care must be taken when implementing custom layers to handle batch dimensions. We will see an application of batching later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A dummy regression dataset\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_x = 5. * torch.randn(100, 2)\n",
    "    real_w = torch.randn(2, 1)\n",
    "    real_b = 2.0\n",
    "    y = train_x.matmul(real_w) + real_b\n",
    "    train_y = y + torch.randn_like(y)\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "real_w, real_b"
   ]
  },
  {
   "source": [
    "### Key Ingredients of training\n",
    "\n",
    "Training neural networks with PyTorch involves a few key ingredients:\n",
    "\n",
    "- Getting the data\n",
    "- Defining the neural network\n",
    "- Picking an optimizer (e.g. Stochastic Gradient Descent, SGD)\n",
    "- Picking an objective function (PyTorch optimizers always minimize, so the objective function must be a loss value to be minimized)\n",
    "- Iteratively stepping through the optimizer using the computed gradients\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LinearRegression(in_size=train_x.size(-1))\n",
    "optim = torch.optim.SGD(net.parameters(), lr=1e-2)\n",
    "ell = nn.MSELoss()\n",
    "epochs = 500\n",
    "\n",
    "loss_trace = []\n",
    "for e in range(epochs):\n",
    "    optim.zero_grad()  ## We must clear the gradient accumulators from previous runs\n",
    "\n",
    "    pred_y = net(train_x)\n",
    "    loss = ell(pred_y, train_y)\n",
    "\n",
    "    loss_trace.append(loss.detach().item())\n",
    "    if (e + 1) % 50 == 0:\n",
    "        print(f'[Epoch {e + 1}] Loss: {loss_trace[-1]}')\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "for p in net.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(loss_trace)), loss_trace)\n",
    "ax.set_xticks(range(0, 501, 50))\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE Loss');"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "A typical successful training schedule will look like this. Such a graph is an indication that our objective has converged to at least a local minima (global if the objective is convex). Note, however, that this may not be sufficient but is a good first diagnostic."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In our new unified view through neural networks, logistic regression is nothing but adding a new sigmoid layer on top of the linear layer. The module below is literally a copy of the previously written `LinearRegression` module, but with the added sigmoid non-linearity through the `nn.Sigmoid()` module.\n",
    "\n",
    "A new module to notice is `nn.Sequential()`. This allows grouping of different modules that need to be applied in sequence, such that we can avoid boilerplate code which computes each layer one-by-one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "### Generate Classification Data\n",
    "\n",
    "We will generate the Two Moons dataset using `scikit-learn`, and make sure we convert them to `torch.Tensor` for subsequent training with PyTorch.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "train_x, train_y = datasets.make_moons(200, noise=0.1)\n",
    "train_x = torch.from_numpy(train_x).float()\n",
    "train_y = torch.from_numpy(train_y).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(train_x[:, 0], train_x[:, 1], c=train_y)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y');"
   ]
  },
  {
   "source": [
    "### Training using the Cross-Entropy Loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LogisticRegression(in_size=train_x.size(-1))\n",
    "optim = torch.optim.SGD(net.parameters(), lr=5e-1)\n",
    "ell = nn.BCELoss()\n",
    "epochs = 1000\n",
    "\n",
    "loss_trace = []\n",
    "for e in range(epochs):\n",
    "    optim.zero_grad()  ## We must clear the gradient accumulators from previous runs\n",
    "\n",
    "    p_y = net(train_x).squeeze(-1)\n",
    "    loss = ell(p_y, train_y.float())\n",
    "\n",
    "    loss_trace.append(loss.detach().item())\n",
    "    if (e + 1) % 50 == 0:\n",
    "        print(f'[Epoch {e + 1}] Loss: {loss_trace[-1]}')\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(loss_trace)), loss_trace)\n",
    "ax.set_xticks(range(0, epochs + 1, 100))\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE Loss');"
   ]
  },
  {
   "source": [
    "### Plotting the predictive surface\n",
    "\n",
    "As expected, we will find that logistic regression with linear predictors can only build linear decision boundaries."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_x, g_y = torch.meshgrid(torch.linspace(-1.5, 2.5, 100), torch.linspace(-1., 1.5, 100))\n",
    "mesh_x = torch.cat([g_x.unsqueeze(-1), g_y.unsqueeze(-1)], axis=-1)\n",
    "with torch.no_grad():\n",
    "    p_mesh = net(mesh_x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.contourf(mesh_x[..., 0], mesh_x[..., 1], p_mesh[..., 0], cmap=plt.cm.coolwarm)\n",
    "ax.scatter(train_x[:, 0], train_x[:, 1], c=train_y, cmap=plt.cm.coolwarm)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y');"
   ]
  },
  {
   "source": [
    "## Logistic Regression with a non-linear neural network\n",
    "\n",
    "Neural networks are powerful in general because they allow non-linearities through _activation functions_. We use the `nn.ReLU()` activation function below, which is applied pointwise as\n",
    "\n",
    "$$\n",
    "ReLU(x) = \\begin{cases}x, \\text{if}~x > 0 \\\\ 0, \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "\n",
    "Such non-linearities allow us non-linear decision boundaries.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_size, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "### Data Loaders\n",
    "\n",
    "While not really necessary here, PyTorch Data Loaders provide a useful abstraction to work with large datasets. We first define a PyTorch dataset, and then wrap it in a data loader."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MoonsDataset(Dataset):\n",
    "    def __init__(self, n=200, noise=0.1):\n",
    "        super().__init__()\n",
    "        train_x, train_y = datasets.make_moons(200, noise=noise)\n",
    "\n",
    "        train_x = torch.from_numpy(train_x).float()\n",
    "        train_y = torch.from_numpy(train_y).int()\n",
    "\n",
    "    def __len__(self):\n",
    "        return train_x.size(0)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return train_x[i], train_y[i]"
   ]
  },
  {
   "source": [
    "We will now train using the DataLoader which will automatically generate _mini-batches_ of data. Another important change to note here is the introduction of _regularization_ using the `weight_decay` argument."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moons = MoonsDataset()\n",
    "loader = DataLoader(moons, batch_size=50, shuffle=True)\n",
    "\n",
    "net = LogisticRegression(in_size=train_x.size(-1))\n",
    "optim = torch.optim.SGD(net.parameters(), lr=5e-1, weight_decay=1e-2)\n",
    "ell = nn.BCELoss()\n",
    "epochs = 1000\n",
    "\n",
    "loss_trace = []\n",
    "for e in range(epochs):\n",
    "    optim.zero_grad()  ## We must clear the gradient accumulators from previous runs\n",
    "\n",
    "    for b_x, b_y in loader:\n",
    "        p_y = net(b_x).squeeze(-1)\n",
    "        loss = ell(p_y, b_y.float())\n",
    "\n",
    "        loss_trace.append(loss.detach().item())\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    if (e + 1) % 100 == 0:\n",
    "        print(f'[Epoch {e + 1}] Loss: {loss_trace[-1]}')"
   ]
  },
  {
   "source": [
    "### A non-linear prediction surface"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_x, g_y = torch.meshgrid(torch.linspace(-1.5, 2.5, 100), torch.linspace(-1., 1.5, 100))\n",
    "mesh_x = torch.cat([g_x.unsqueeze(-1), g_y.unsqueeze(-1)], axis=-1)\n",
    "with torch.no_grad():\n",
    "    p_mesh = net(mesh_x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.contourf(mesh_x[..., 0], mesh_x[..., 1], p_mesh[..., 0], cmap=plt.cm.coolwarm)\n",
    "ax.scatter(train_x[:, 0], train_x[:, 1], c=train_y, cmap=plt.cm.coolwarm)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y');"
   ]
  },
  {
   "source": [
    "The predicition surface is non-linear and clearly separates our two classes cleanly!\n",
    "\n",
    "What would happen if we set `weight_decay` to a higher value? What about a lower value?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}