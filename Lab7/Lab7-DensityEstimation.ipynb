{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Claw Function\n",
    "\n",
    "We consider the following density, derived from a mixture of six Gaussians,\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{2} \\mathcal{N}(0, 1) + \\frac{1}{10} \\sum_{j=0}^4 \\mathcal{N}((j/2)-1,1/100)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claw(x):\n",
    "    sum_pdf = sum([stats.norm(loc=(j/2-1), scale=0.1).pdf(x) for j in range(5)])\n",
    "    return stats.norm().pdf(x) / 2 + sum_pdf / 10\n",
    "\n",
    "def sample_claw(n=1000):\n",
    "    '''Generate samples via ancestral sampling (i.e. follow the generative process).\n",
    "    '''\n",
    "    dist = [stats.norm()] + [stats.norm(loc=(j/2-1), scale=0.1) for j in range(5)]\n",
    "    c_dist = stats.multinomial(1, np.array([1/2, 1/10, 1/10, 1/10, 1/10, 1/10]))\n",
    "    c_sample = c_dist.rvs(size=n).argmax(axis=-1)\n",
    "    return np.array([dist[c].rvs() for c in c_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "pdf = claw(x)\n",
    "\n",
    "ax.plot(x, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "samples = sample_claw()[:, np.newaxis]\n",
    "ax.scatter(samples, claw(samples), s=5, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big picture question that density estimation answers is -- given samples from some underlying generative process, can we recover the process? We are moving from the supervised learning setting to an unsupervised learning setting. Now, we must discover the structure in the data without some form of ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "We have infact always been using the simplest density estimator -- the histogram!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10,10), nrows=2, ncols=2, sharex=True)\n",
    "\n",
    "bins = [250, 100, 50, 10]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[i,j].hist(samples, bins=bins[i*2 + j])\n",
    "\n",
    "        axes[i,j].set_title(f'Bins = {bins[i*2 + j]}')\n",
    "        axes[i,j].set_ylabel('Count')\n",
    "        if i > 0:\n",
    "            axes[i,j].set_xlabel(r'$x$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, having finer bins can reveal finer structures with sufficient number of samples. This can, however, quickly turn bad because choosing bins in higher dimensional spaces is equivalent to picking hypercubes, which is tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Density Estimators\n",
    "\n",
    "These are non-parametric density estimators, often useful mostly in low dimensions. We place _smoothing kernels_ $K$ centered at each sample, where the coverage is specified by a bandwidth parameter $h$. In general, the estimated density function is then given by\n",
    "\n",
    "$$\n",
    "\\widehat{p}(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(\\lvert x - x_i \\rvert)\n",
    "$$\n",
    "\n",
    "A common choice of a _smoothing kernels_ (i.e $\\int K(z) dz = 1$) is the Gaussian kernel,\n",
    "\n",
    "$$\n",
    "K_h(z) = \\frac{1}{\\sqrt{2\\pi h}}\\exp{\\left\\{-\\frac{z^2}{2h} \\right\\} }\n",
    "$$\n",
    "\n",
    "The bandwidth parameter helps us balance the bias-variance trade-off.\n",
    "\n",
    "**NOTE**: You may encounter the term kernel used in different contexts across machine learning, e.g. a _positive-definite kernel_, _Mercer kernel_, etc. These are different from _smoothing kernels_ used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10,10), nrows=2, ncols=2, sharex=True, sharey=True)\n",
    "\n",
    "bw = [0.005, 0.05 , 0.1, 1.]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        kde = KernelDensity(bandwidth=bw[i * 2 + j])\n",
    "        kde.fit(samples)\n",
    "\n",
    "        axes[i,j].plot(x[:,np.newaxis], np.exp(kde.score_samples(x[:,np.newaxis])))\n",
    "\n",
    "        axes[i,j].set_title(fr'$h = {bw[i * 2 + j]}$')\n",
    "        if i > 0:\n",
    "            axes[i,j].set_xlabel(fr'$x$')\n",
    "        if j < 1:\n",
    "            axes[i,j].set_ylabel(fr'$p(x)$')\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, we see that on one hand smaller bandwidths create overly complicated distributions, but larger bandwidths create overly simplistic distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models\n",
    "\n",
    "Gaussian Mixture Models (GMMs) are a class of discrete _latent_ variable models, described by the following marginal density over the input data\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_{k=1}^K p(z = k) p(x\\mid z = k)\n",
    "$$\n",
    "\n",
    "where $p(x\\mid z = k) = \\mathcal{N}(x; \\mu_k, \\Sigma_k)$ is the $k^{\\mathrm{th}}$ Gaussian component in the mixture. Clearly, $0 < p(z=k) < 1$, and $\\sum_{k=1}^K p(z = k) = 1$. In theory, GMMs can approximate any smooth density, i.e. they are universal approximators of density functions. In practice, however, this property is harder to utilize due to light tails and being \"too smooth\".\n",
    "\n",
    "For a given set of samples $\\left\\{x_i\\right\\}_{i=1}^N$, we aim to maximize the $\\log$-likelihood using the density function $p(x)$. This involves finding each mixing paramter $p(z = k)$, and the mean and covariances for each $p(x \\mid z=k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Faithful Dataset\n",
    "\n",
    "This dataset has been pre-processed to be zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X = pd.read_csv('oldfaithful.csv').values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve a constrained optimization problem via the method of Lagrange multipliers. It is, however, not feasible to maximize this in closed-form. Nevertheless, we can build an iterative procedure after some algebraic manipulations, which boils down to the alternating between the following steps:\n",
    "\n",
    "**E step**:\n",
    "\n",
    "$$\n",
    "\\gamma(z_{ik}) \\propto p(z=k)p(x_i \\mid z = k)\n",
    "$$\n",
    "\n",
    "**M step**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "n_k &= \\sum_{i=1}^n \\gamma(z_{ik}) \\\\\n",
    "\\mu_{k}^{\\mathrm{new}} &= \\frac{1}{n_k} \\sum_{i=1}^n \\gamma(z_{ik}) x_i \\\\\n",
    "\\Sigma_{k}^{\\mathrm{new}} &= \\frac{1}{n_k} \\sum_{i=1}^n \\gamma(z_{ik}) (x_i - \\mu_{k}^{\\mathrm{new}})(x_i - \\mu_{k}^{\\mathrm{new}})^T \\\\\n",
    "p^{\\mathrm{new}}(z = k) &= \\frac{n_k}{n}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Intuitively, these update steps first compute the contribution of each cluster component towards explaining a datapoint. Finally, using these contribution weights, we compute the new mixing coefficients, and the means and covariances.\n",
    "\n",
    "This iterative procedure is known as **Expectation-Maximization** (EM). The procedure is more generally applicable than just for GMMs -- typically for problems where maximizing the _complete_ distribution (the full joint) is easier than maximizing the _marginal_ (the algorithm was introduced for a missing data setting first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def gmm_lik(X, pi, mu, cov):\n",
    "    '''\n",
    "    X: N x D\n",
    "    pi: K\n",
    "    mu: K x D\n",
    "    cov: K x D x D\n",
    "    '''\n",
    "    dists = [stats.multivariate_normal(mean=mu[k], cov=cov[k]) for k in range(K)]\n",
    "    pdf = np.c_[[dist.pdf(X) for dist in dists]].T  ## N x K\n",
    "    lik = np.sum(pdf * pi[np.newaxis, :], axis=-1)\n",
    "    return lik\n",
    "\n",
    "def expectation(X, pi, mu, cov):\n",
    "    '''\n",
    "    X: N x D\n",
    "    pi: K\n",
    "    mu: K x D\n",
    "    cov: K x D x D\n",
    "    '''\n",
    "    K = len(pi)\n",
    "    dists = [stats.multivariate_normal(mean=mu[k], cov=cov[k]) for k in range(K)]\n",
    "    pdf = np.c_[[dist.pdf(X) for dist in dists]].T  ## N x K\n",
    "\n",
    "    gamma = pdf * pi[np.newaxis, ...]  ## N x K\n",
    "    gamma = gamma / np.sum(gamma, axis=-1, keepdims=True)\n",
    "\n",
    "    return gamma\n",
    "\n",
    "def maximization(X, gamma):\n",
    "    '''\n",
    "    X: N x D\n",
    "    gamma: N x K\n",
    "    '''\n",
    "    N, K = gamma.shape\n",
    "    N_k = np.sum(gamma, axis=0) ## K\n",
    "    \n",
    "    mu_new = np.c_[[np.sum(gamma[:, k, np.newaxis] * X, axis=0) / N_k[k] for k in range(K)]]  ## K x D\n",
    "\n",
    "    X_c = [X - mu_new[k][np.newaxis, :] for k in range(K)]\n",
    "    cov_new = np.c_[[np.matmul(X_c[k].T, gamma[:, k, np.newaxis] * X_c[k]) / N_k[k] for k in range(K)]]  ## K x D x D\n",
    "\n",
    "    pi = N_k / N\n",
    "\n",
    "    return pi, mu_new, cov_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training of GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "K = 2\n",
    "D = 2\n",
    "pi = np.ones(K) / K\n",
    "\n",
    "# mu = 2. * np.random.randn(K, D)\n",
    "mu = np.array([[1., -1.], [-1., 1.5]])\n",
    "\n",
    "# kmeans = KMeans(n_clusters=K).fit(X)\n",
    "# mu = kmeans.cluster_centers_\n",
    "\n",
    "cov = np.c_[[np.eye(D,D) for k in range(K)]]\n",
    "\n",
    "ckpts_i = [1, 5, 10, 15, 20]\n",
    "ckpts = []\n",
    "\n",
    "for i in range(20):\n",
    "    ## store desired checkpoints for plotting.\n",
    "    if i + 1 in ckpts_i:\n",
    "        ckpts.append((np.copy(pi), np.copy(mu), np.copy(cov)))\n",
    "\n",
    "    gamma = expectation(X, pi, mu, cov)\n",
    "    pi, mu, cov = maximization(X, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(5,5 * len(ckpts_i)), nrows=len(ckpts_i), sharex=True, sharey=True)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-2.1, 2.1, 200), np.linspace(-2.1, 2.1, 200))\n",
    "mesh_X = np.concatenate([xx[..., np.newaxis], yy[..., np.newaxis]], axis=-1).reshape(-1, 2)\n",
    "\n",
    "for ax, i, (pi, mu, cov) in zip(axes, ckpts_i, ckpts):\n",
    "    ax.contourf(xx, yy, gmm_lik(mesh_X, pi, mu, cov).reshape(*xx.shape), cmap=plt.cm.coolwarm)\n",
    "    X_lik = gmm_lik(X, pi, mu, cov)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=X_lik, s=7, cmap=plt.cm.viridis)\n",
    "    ax.set_title(f'Iteration {i}')\n",
    "    ax.set_ylabel(fr'$p(x)$')\n",
    "    ax.set_aspect('equal')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting GMMs with scikit-learn\n",
    "\n",
    "`scikit-learn` implements the above algorithm as well. In particular, it allows us to use k-Means cluster initialization for potentially faster convergence. It also allows for other kinds of parametrizations, e.g. a \"tied\" parametrization which shares a \"full\" covariance matrix across all components of the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(5,20), nrows=4, sharex=True, sharey=True)\n",
    "\n",
    "for ax, ctype in zip(axes, ['full', 'tied', 'diag', 'spherical']):\n",
    "    gm = GaussianMixture(n_components=K, covariance_type=ctype).fit(X)\n",
    "\n",
    "    ax.contourf(xx, yy, np.exp(gm.score_samples(mesh_X)).reshape(*xx.shape), cmap=plt.cm.coolwarm)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=np.exp(gm.score_samples(X)), s=7, cmap=plt.cm.viridis)\n",
    "    ax.set_title(f'Covariance type = {ctype}')\n",
    "    ax.set_ylabel(fr'$p(x)$')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
