{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "In this lab we will look at linear dimensionality reduction via PCA, and then look at non-linear dimensionality reduction via (variational) auto-encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MIT CBCL Faces Database from [here](https://github.com/HyTruongSon/Pattern-Classification/blob/master/MIT-CBCL-database/svm.train.normgrey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    raw_data = []\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f.readlines()[2:]:\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            if int(line.split()[-1]) == -1:\n",
    "                # not face, skip\n",
    "                continue\n",
    "            else:\n",
    "                raw_data.append([float(yy) for yy in line.split()[:-1]])\n",
    "    return np.array(raw_data)\n",
    "\n",
    "data = load_data('svm.train.normgrey')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_idx = np.random.permutation(len(data))[:10]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10, 3), nrows=2, ncols=5)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        k = i * 5 + j\n",
    "        axes[i, j].imshow(data[viz_idx[k]].reshape(19, 19), cmap=plt.cm.gray, interpolation='bilinear')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PCA\n",
    "\n",
    "Dimensionality reduction is essentially a basis selection problem. PCA aims to identify the \"best\" linear transform, such that we start viewing the data along the directions of maximum variance. The key intuition here is that directions that have high variance in the data space, also have the highest signal to \"learn\" from. \n",
    "\n",
    "As it turns out, the principal components that best describe this reconstruction are the eigenvectors of the covariance matrix. Equivalently, these are the right-singular vectors of the data matrix whose each row represents a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data - np.mean(data, axis=0, keepdims=True)\n",
    "W, S, _ = np.linalg.svd(X.T)\n",
    "S.shape, W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Eigen Faces\n",
    "\n",
    "The columns of $V$ form the eigenfaces. We take the first $q$ column vectors to form the \"eigenbasis\" of face vectors, i.e. eigen faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 100\n",
    "W = W[:, :q]\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10, 5), nrows=2, ncols=5)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        k = i * 5 + j\n",
    "        axes[i, j].imshow(W[:, k].reshape(19, 19), cmap=plt.cm.gray, interpolation='bilinear')\n",
    "        axes[i, j].axis('off')\n",
    "        axes[i, j].set_title(f'Dimension = {k}')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we see that different structures/features are highlighted in each of the eigen faces.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a new face\n",
    "\n",
    "How about creating a new face which contains a smile and a moustache? Via visual inspection, it seems like dimension 4 and dimension 7 can combine to give us a moustache person with a smile. Abstractly, this is how we've always thought of vector space. This is abstract vector spaces applied to the vector space of faces!\n",
    "\n",
    "**NOTE**: This is purely by inspection for now but one can imagine how controlled generation can happen if you control for such facial features, and attach semantics to them. Getting nice faces may be much harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = W[:, 4] + W[:, 7]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(gen.reshape(19, 19), cmap=plt.cm.gray, interpolation='bilinear')\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained Variance\n",
    "\n",
    "How much of the total variance along each of the principle components do we explain by picking the first $q$ components only? Remember that we have essentially diagonalized the system into orthogonal components, such that all covariances are zero. The explained variance can then be deduced by the diagonal singular value matrix $S$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Explained variance: {np.sum(S[:q]) / np.sum(S) * 100:.2f}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders\n",
    "\n",
    "Variational Autoencoders (VAEs) provide us a way to do non-linear dimensionality reduction. We will compress the information contained in the MNIST dataset into two-dimensional vectors. By interpolating this 2-D space, we will be able to visualized whether our learned latent variables have capture enough information about the digits.\n",
    "\n",
    "Formally, we posit that the generative process behind each MNIST digit involves a continuous latent variables $z \\sim \\mathcal{N}(\\mu, \\Sigma)$. For simplicity, we assume that the covariance is diagonal. The digit $x$ is generate via an appropriate model $p(x \\mid z)$. We assume a standard normal prior over $z$, as $p(z) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "We first define a _variational_ encoder $q(z \\mid x) = \\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi(x))$ (this is our variational posterior). Note that both the mean and the diagonal covariance are parameterized by some parameters, collectively denoted by $\\phi$. In our case, this will be a simple neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_size, latent_size):\n",
    "        '''\n",
    "        Arguments:\n",
    "            in_size (int): Dimension of inputs\n",
    "            latent_size (int): Dimension of final latent space.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(in_size, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu_net = nn.Linear(512, latent_size)\n",
    "        self.sigma_net = nn.Sequential(\n",
    "            nn.Linear(512, latent_size),\n",
    "            nn.Softplus()  # variance has to be positive\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Arguments:\n",
    "            x (Tensor): Shape batch_size x in_size\n",
    "        Returns:\n",
    "            mu (Tensor): Variational mean, shape (batch_size x latent_size)\n",
    "            sigma (Tensor): Variational covariance, shape (batch_size x latent_size)\n",
    "        '''\n",
    "        encoded = self.enc(x)\n",
    "        mu = self.mu_net(encoded)\n",
    "        sigma = self.sigma_net(encoded)\n",
    "\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "For the decoder, we will model $p(x \\mid z)$ as $\\mathcal{N}(\\mu_\\theta(z), \\sigma^2)$ independently for each pixel of the observed MNIST image sample. Note that here only the mean is parameterized for simplicity. In this case, our decoder will mirror the encoder and reverse the process for reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, out_size):\n",
    "        '''\n",
    "        Arguments:\n",
    "            latent_size (int): Dimension of final latent space.\n",
    "            out_size (int): Dimension of output vectors\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(latent_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        '''\n",
    "        Arguments:\n",
    "            z (Tensor): batch_size x latent_size\n",
    "        Returns:\n",
    "            x_hat (Tensor): batch_size x in_size\n",
    "        '''\n",
    "\n",
    "        x_hat = self.dec(z)\n",
    "\n",
    "        return x_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training VAEs\n",
    "\n",
    "VAEs are trained by minimizing a regularized reconstruction loss. This can in fact be derived as a consequence of maximizing the evidence lower bound $\\mathcal{L}$.\n",
    "\n",
    "$$\n",
    "\\log{p(\\mathbf{X}\\mid\\theta)} = \\underbrace{\\mathbb{E}_q[\\log{p(\\mathbf{X},\\mathbf{Z}\\mid\\theta)} - \\log{q(\\mathbf{Z})}]}_{\\mathcal{L}(q,\\theta)} + \\mathrm{KL}(q(\\mathbf{Z}) \\mid\\mid  p(\\mathbf{Z}\\mid\\mathbf{X},\\theta))\n",
    "$$\n",
    "\n",
    "For any approximate posterior $q$, the lower bound can be rewritten as,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(q,\\theta) = \\underbrace{\\mathbb{E}_{q(z\\mid x)}\\left[ p(\\mathbf{X}\\mid \\mathbf{Z}, \\theta) \\right]}_{\\text{Reconstruction loss}} - \\overbrace{\\mathrm{KL}(q(\\mathbf{Z} \\mid \\mathbf{X}) \\mid\\mid p(\\mathbf{Z}))}^{\\text{Regularizer}}\n",
    "$$\n",
    "\n",
    "When the output is Gaussian, this reconstruction loss is just the squared error as we've observed a few times before. This is what we will use below. Further, the KL-divergence between two Gaussians can be easily computed in closed form. This loss is easily computable and we can optimize using gradient descent.\n",
    "\n",
    "**NOTE**: While the specifics of this loss are interesting to understand, it is enough to realize that our intuitive way of encoding images into low-dimensional vectors via simply a reconstruction loss can be justified theoretically. As generally done, we bias the solutions via a regularizer. Previously, we have been using the L2-norm of the parameters as a regularizer. In this case, we use the $KL$-divergence, which simply says don't go too far away from the prior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELoss(nn.Module):\n",
    "    def __init__(self, beta=1.):\n",
    "        super().__init__()\n",
    "        self.beta = 1.\n",
    "\n",
    "    def forward(self, x, x_hat, mu, sigma):\n",
    "        mse_loss = (x - x_hat).pow(2).sum()\n",
    "        kl_loss = (sigma.pow(2) + mu.pow(2) - 1.0 - 2. * sigma.log()).div(2.).sum()\n",
    "\n",
    "        return mse_loss + self.beta * kl_loss\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_size, latent_size)\n",
    "        self.decoder = Decoder(latent_size, in_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encoder(x)\n",
    "        z = mu + sigma * torch.randn_like(mu)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist = MNIST(root='/tmp', download=True, train=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else None\n",
    "\n",
    "vae = VAE(784, 2).to(device)\n",
    "criterion = VAELoss(beta=1.)\n",
    "optim = torch.optim.Adam(vae.parameters())\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    for b_x, _ in tqdm(DataLoader(mnist, batch_size=2000), leave=False):\n",
    "        B, *_ = b_x.shape\n",
    "        x = b_x.view(B, -1).to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        x_hat, mu, sigma = vae(x)\n",
    "        loss = criterion(x, x_hat, mu, sigma)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the latent space\n",
    "\n",
    "Now let us take any two interesting samples from the dataset (e.g. a digit 0 and digit 8). We will interpolate between the two digits by first encoding these samples into the latent space, and along the direction between the two in the same space (and decoding back into the original space for visualization).\n",
    "\n",
    "Formally, consider two samples $x_0$ and $x_1$ and corresponding mean encodings $z_0$ and $z_1$, a convex interpolation in the latent space will be along the line, for a scalar parameter $t \\in [0, 1]$.\n",
    "\n",
    "$$\n",
    "z = z_0 + t \\times (z_1 - z_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  x_0 = mnist.data[mnist.targets == 0][0].to(device).view(1, -1).float() / 255\n",
    "  x_1 = mnist.data[mnist.targets == 8][0].to(device).view(1, -1).float() / 255\n",
    "  z_0, _ = vae.encoder(x_0)\n",
    "  z_1, _ = vae.encoder(x_1)\n",
    "\n",
    "  z = (z_0 + torch.linspace(0, 1, 20).to(device).unsqueeze(-1) * (z_1 - z_0))\n",
    "  x_hat = vae.decoder(z)\n",
    "\n",
    "x_hat = x_hat.reshape(z.size(0), 28, 28).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20, 5), ncols=x_hat.shape[0])\n",
    "for i in range(x_hat.shape[0]):\n",
    "  axes[i].imshow(x_hat[i], cmap=plt.cm.gray)\n",
    "  axes[i].axis('off')\n",
    "  axes[i].set_aspect('equal')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
